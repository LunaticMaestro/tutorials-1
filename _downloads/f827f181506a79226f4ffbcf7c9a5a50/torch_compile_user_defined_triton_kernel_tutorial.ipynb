{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using User-Defined Triton Kernels with `torch.compile`\n",
    "======================================================\n",
    "\n",
    "**Author:** [Oguz Ulgen](https://github.com/oulgen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-defined Triton kernels can be used to optimize specific parts of\n",
    "your model\\'s computation. These kernels are written in Triton\\'s\n",
    "language, which is designed to make it easier to achieve peak hardware\n",
    "performance. By using user-defined Triton kernels with `torch.compile`,\n",
    "you can integrate these optimized computations into your PyTorch model,\n",
    "potentially achieving significant performance improvements.\n",
    "\n",
    "This recipes demonstrates how you can use user-defined Triton kernels\n",
    "with `torch.compile`.\n",
    "\n",
    "Prerequisites\n",
    "=============\n",
    "\n",
    "Before starting this recipe, make sure that you have the following:\n",
    "\n",
    "-   Basic understanding of `torch.compile` and Triton. See:\n",
    "    -   [torch.compiler API\n",
    "        documentation](https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler)\n",
    "    -   [Introduction to\n",
    "        torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n",
    "    -   [Triton language\n",
    "        documentation](https://triton-lang.org/main/index.html)\n",
    "-   PyTorch 2.3 or later\n",
    "-   A GPU that supports Triton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils._triton import has_triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Usage\n",
    "===========\n",
    "\n",
    "In this example, we will use a simple vector addition kernel from the\n",
    "Triton documentation with `torch.compile`. For reference, see [Triton\n",
    "documentation](https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not has_triton():\n",
    "    print(\"Skipping because triton is not supported on this device.\")\n",
    "else:\n",
    "    import triton\n",
    "    from triton import language as tl\n",
    "\n",
    "    @triton.jit\n",
    "    def add_kernel(\n",
    "        in_ptr0,\n",
    "        in_ptr1,\n",
    "        out_ptr,\n",
    "        n_elements,\n",
    "        BLOCK_SIZE: \"tl.constexpr\",\n",
    "    ):\n",
    "        pid = tl.program_id(axis=0)\n",
    "        block_start = pid * BLOCK_SIZE\n",
    "        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_elements\n",
    "        x = tl.load(in_ptr0 + offsets, mask=mask)\n",
    "        y = tl.load(in_ptr1 + offsets, mask=mask)\n",
    "        output = x + y\n",
    "        tl.store(out_ptr + offsets, output, mask=mask)\n",
    "\n",
    "    @torch.compile(fullgraph=True)\n",
    "    def add_fn(x, y):\n",
    "        output = torch.zeros_like(x)\n",
    "        n_elements = output.numel()\n",
    "        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "        add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=4)\n",
    "        return output\n",
    "\n",
    "    x = torch.randn(4, device=\"cuda\")\n",
    "    y = torch.randn(4, device=\"cuda\")\n",
    "    out = add_fn(x, y)\n",
    "    print(f\"Vector addition of\\nX:\\t{x}\\nY:\\t{y}\\nis equal to\\n{out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Usage\n",
    "==============\n",
    "\n",
    "Triton\\'s autotune feature is a powerful tool that automatically\n",
    "optimizes the configuration parameters of your Triton kernels. It\n",
    "explores a range of possible configurations and selects the one that\n",
    "delivers the best performance for your specific use case.\n",
    "\n",
    "When used with `torch.compile`, `triton.autotune` can help ensure that\n",
    "your PyTorch model is running as efficiently as possible. Here is an\n",
    "example of using `torch.compile` and `triton.autotune`.\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p><code>torch.compile</code> only supports configs and key arguments to <code>triton.autotune</code>.</p>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not has_triton():\n",
    "    print(\"Skipping because triton is not supported on this device.\")\n",
    "else:\n",
    "    import triton\n",
    "    from triton import language as tl\n",
    "\n",
    "    @triton.autotune(\n",
    "        configs=[\n",
    "            triton.Config({\"BLOCK_SIZE\": 4}, num_stages=3, num_warps=8),\n",
    "            triton.Config({\"BLOCK_SIZE\": 4}, num_stages=4, num_warps=4),\n",
    "            triton.Config({\"BLOCK_SIZE\": 2}, num_stages=3, num_warps=8),\n",
    "            triton.Config({\"BLOCK_SIZE\": 2}, num_stages=4, num_warps=4),\n",
    "        ],\n",
    "        key=[],\n",
    "    )\n",
    "    @triton.jit\n",
    "    def add_kernel_autotuned(\n",
    "        in_ptr0,\n",
    "        in_ptr1,\n",
    "        out_ptr,\n",
    "        n_elements,\n",
    "        BLOCK_SIZE: \"tl.constexpr\",\n",
    "    ):\n",
    "        pid = tl.program_id(axis=0)\n",
    "        block_start = pid * BLOCK_SIZE\n",
    "        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_elements\n",
    "        x = tl.load(in_ptr0 + offsets, mask=mask)\n",
    "        y = tl.load(in_ptr1 + offsets, mask=mask)\n",
    "        output = x + y\n",
    "        tl.store(out_ptr + offsets, output, mask=mask)\n",
    "\n",
    "    @torch.compile(fullgraph=True)\n",
    "    def add_fn(x, y):\n",
    "        output = torch.zeros_like(x)\n",
    "        n_elements = output.numel()\n",
    "        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "        add_kernel_autotuned[grid](x, y, output, n_elements)\n",
    "        return output\n",
    "\n",
    "    x = torch.randn(4, device=\"cuda\")\n",
    "    y = torch.randn(4, device=\"cuda\")\n",
    "    out = add_fn(x, y)\n",
    "    print(f\"Vector addition of\\nX:\\t{x}\\nY:\\t{y}\\nis equal to\\n{out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composibility and Limitations\n",
    "=============================\n",
    "\n",
    "As of PyTorch 2.3, the support for user-defined Triton kernels in\n",
    "`torch.compile` includes dynamic shapes, `torch.autograd.Function`, JIT\n",
    "inductor, and AOT inductor. You can use these features together to build\n",
    "complex, high-performance models.\n",
    "\n",
    "However, there are certain limitations to be aware of:\n",
    "\n",
    "-   **Tensor Subclasses:** Currently, there is no support for tensor\n",
    "    subclasses and other advanced features.\n",
    "-   **Triton Features:** While `triton.heuristics` can be used either\n",
    "    standalone or before `triton.autotune`, it cannot be used after\n",
    "    `triton.autotune`. This implies that if `triton.heuristics` and\n",
    "    `triton.autotune` are to be used together, `triton.heuristics` must\n",
    "    be used first.\n",
    "\n",
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this recipe, we explored how to utilize user-defined Triton kernels\n",
    "with `torch.compile`. We delved into the basic usage of a simple vector\n",
    "addition kernel and advanced usage involving Triton\\'s autotune feature.\n",
    "We also discussed the composability of user-defined Triton kernels with\n",
    "other PyTorch features and highlighted some current limitations.\n",
    "\n",
    "See Also\n",
    "========\n",
    "\n",
    "-   [Compiling the\n",
    "    Optimizers](https://pytorch.org/tutorials/recipes/compiling_optimizer.html)\n",
    "-   [Implementing High-Performance Transformers with Scaled Dot Product\n",
    "    Attention](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
