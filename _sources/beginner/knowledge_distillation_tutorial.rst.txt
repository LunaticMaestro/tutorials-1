
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "beginner/knowledge_distillation_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_beginner_knowledge_distillation_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_beginner_knowledge_distillation_tutorial.py:


Knowledge Distillation Tutorial
===============================
**Author**: `Alexandros Chariton <https://github.com/AlexandrosChrtn>`_

.. GENERATED FROM PYTHON SOURCE LINES 9-33

Knowledge distillation is a technique that enables knowledge transfer from large, computationally expensive
models to smaller ones without losing validity. This allows for deployment on less powerful
hardware, making evaluation faster and more efficient. 

In this tutorial, we will run a number of experiments focused at improving the accuracy of a
lightweight neural network, using a more powerful network as a teacher.
The computational cost and the speed of the lightweight network will remain unaffected,
our intervention only focuses on its weights, not on its forward pass.
Applications of this technology can be found in devices such as drones or mobile phones.
In this tutorial, we do not use any external packages as everything we need is available in ``torch`` and
``torchvision``.

In this tutorial, you will learn:

- How to modify model classes to extract hidden representations and use them for further calculations
- How to modify regular train loops in PyTorch to include additional losses on top of, for example, cross-entropy for classification 
- How to improve the performance of lightweight models by using more complex models as teachers

Prerequisites
~~~~~~~~~~~~~

* 1 GPU, 4GB of memory
* PyTorch v2.0 or later 
* CIFAR-10 dataset (downloaded by the script and saved in a directory called ``/data``)

.. GENERATED FROM PYTHON SOURCE LINES 33-43

.. code-block:: default


    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torchvision.transforms as transforms
    import torchvision.datasets as datasets

    # Check if GPU is available, and if not, use the CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")








.. GENERATED FROM PYTHON SOURCE LINES 44-68

Loading CIFAR-10
----------------
CIFAR-10 is a popular image dataset with ten classes. Our objective is to predict one of the following classes for each input image.

.. figure:: /../_static/img/cifar10.png 
   :align: center

   Example of CIFAR-10 images

The input images are RGB, so they have 3 channels and are 32x32 pixels. Basically, each image is described by 3 x 32 x 32 = 3072 numbers ranging from 0 to 255.
A common practice in neural networks is to normalize the input, which is done for multiple reasons,
including avoiding saturation in commonly used activation functions and increasing numerical stability.
Our normalization process consists of subtracting the mean and dividing by the standard deviation along each channel.
The tensors "mean=[0.485, 0.456, 0.406]" and "std=[0.229, 0.224, 0.225]" were already computed,
and they represent the mean and standard deviation of each channel in the
predefined subset of CIFAR-10 intended to be the training set.
Notice how we use these values for the test set as well, without recomputing the mean and standard deviation from scratch.
This is because the network was trained on features produced by subtracting and dividing the numbers above, and we want to maintain consistency.
Furthermore, in real life, we would not be able to compute the mean and standard deviation of the test set since,
under our assumptions, this data would not be accessible at that point.

As a closing point, we often refer to this held-out set as the validation set, and we use a separate set,
called the test set, after optimizing a model's performance on the validation set.
This is done to avoid selecting a model based on the greedy and biased optimization of a single metric.

.. GENERATED FROM PYTHON SOURCE LINES 68-79

.. code-block:: default


    # Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.
    transforms_cifar = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # Loading the CIFAR-10 dataset:
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)
    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz

      0%|          | 0/170498071 [00:00<?, ?it/s]
      0%|          | 327680/170498071 [00:00<00:53, 3210582.89it/s]
      0%|          | 851968/170498071 [00:00<00:38, 4393512.71it/s]
      1%|          | 1376256/170498071 [00:00<00:35, 4748046.13it/s]
      1%|1         | 1900544/170498071 [00:00<00:34, 4846385.65it/s]
      1%|1         | 2424832/170498071 [00:00<00:34, 4872609.28it/s]
      2%|1         | 2949120/170498071 [00:00<00:34, 4920197.08it/s]
      2%|2         | 3473408/170498071 [00:00<00:33, 5005428.71it/s]
      2%|2         | 4030464/170498071 [00:00<00:32, 5079952.34it/s]
      3%|2         | 4554752/170498071 [00:00<00:32, 5093654.28it/s]
      3%|2         | 5079040/170498071 [00:01<00:32, 5123562.83it/s]
      3%|3         | 5603328/170498071 [00:01<00:32, 5087000.25it/s]
      4%|3         | 6127616/170498071 [00:01<00:32, 5067438.61it/s]
      4%|3         | 6651904/170498071 [00:01<00:32, 5002011.50it/s]
      4%|4         | 7176192/170498071 [00:01<00:32, 5028909.90it/s]
      5%|4         | 7700480/170498071 [00:01<00:32, 5086073.95it/s]
      5%|4         | 8224768/170498071 [00:01<00:31, 5084264.95it/s]
      5%|5         | 8749056/170498071 [00:01<00:31, 5112970.57it/s]
      5%|5         | 9273344/170498071 [00:01<00:31, 5075787.67it/s]
      6%|5         | 9797632/170498071 [00:01<00:31, 5065720.61it/s]
      6%|6         | 10321920/170498071 [00:02<00:31, 5011776.91it/s]
      6%|6         | 10846208/170498071 [00:02<00:31, 5012428.98it/s]
      7%|6         | 11370496/170498071 [00:02<00:31, 4985674.71it/s]
      7%|6         | 11894784/170498071 [00:02<00:31, 5006650.85it/s]
      7%|7         | 12419072/170498071 [00:02<00:31, 4966262.85it/s]
      8%|7         | 12943360/170498071 [00:02<00:31, 4976190.77it/s]
      8%|7         | 13467648/170498071 [00:02<00:31, 4985919.30it/s]
      8%|8         | 13991936/170498071 [00:02<00:31, 4936315.89it/s]
      9%|8         | 14516224/170498071 [00:02<00:31, 4965747.20it/s]
      9%|8         | 15040512/170498071 [00:03<00:31, 4942783.85it/s]
      9%|9         | 15564800/170498071 [00:03<00:31, 4891465.52it/s]
      9%|9         | 16056320/170498071 [00:03<00:32, 4735249.48it/s]
     10%|9         | 16580608/170498071 [00:03<00:31, 4844466.45it/s]
     10%|#         | 17072128/170498071 [00:03<00:31, 4797436.27it/s]
     10%|#         | 17563648/170498071 [00:03<00:31, 4800083.15it/s]
     11%|#         | 18055168/170498071 [00:03<00:31, 4796293.11it/s]
     11%|#         | 18546688/170498071 [00:03<00:32, 4735614.87it/s]
     11%|#1        | 19038208/170498071 [00:03<00:31, 4776204.31it/s]
     11%|#1        | 19529728/170498071 [00:03<00:31, 4742019.17it/s]
     12%|#1        | 20021248/170498071 [00:04<00:31, 4760181.48it/s]
     12%|#2        | 20512768/170498071 [00:04<00:31, 4767403.08it/s]
     12%|#2        | 21004288/170498071 [00:04<00:31, 4760500.13it/s]
     13%|#2        | 21495808/170498071 [00:04<00:31, 4801398.86it/s]
     13%|#2        | 21987328/170498071 [00:04<00:31, 4764250.62it/s]
     13%|#3        | 22478848/170498071 [00:04<00:30, 4795585.08it/s]
     13%|#3        | 22970368/170498071 [00:04<00:31, 4755240.58it/s]
     14%|#3        | 23461888/170498071 [00:04<00:30, 4797922.74it/s]
     14%|#4        | 23953408/170498071 [00:04<00:30, 4788576.04it/s]
     14%|#4        | 24444928/170498071 [00:04<00:30, 4791023.98it/s]
     15%|#4        | 24936448/170498071 [00:05<00:30, 4778301.05it/s]
     15%|#4        | 25427968/170498071 [00:05<00:30, 4798093.95it/s]
     15%|#5        | 25919488/170498071 [00:05<00:30, 4812111.72it/s]
     15%|#5        | 26411008/170498071 [00:05<00:30, 4760941.36it/s]
     16%|#5        | 26902528/170498071 [00:05<00:30, 4704238.35it/s]
     16%|#6        | 27394048/170498071 [00:05<00:30, 4643016.36it/s]
     16%|#6        | 27885568/170498071 [00:05<00:30, 4622843.50it/s]
     17%|#6        | 28377088/170498071 [00:05<00:30, 4603096.61it/s]
     17%|#6        | 28868608/170498071 [00:05<00:30, 4596849.08it/s]
     17%|#7        | 29360128/170498071 [00:06<00:30, 4595520.16it/s]
     18%|#7        | 29851648/170498071 [00:06<00:30, 4599196.75it/s]
     18%|#7        | 30343168/170498071 [00:06<00:30, 4616606.82it/s]
     18%|#8        | 30834688/170498071 [00:06<00:30, 4581278.74it/s]
     18%|#8        | 31326208/170498071 [00:06<00:30, 4586069.43it/s]
     19%|#8        | 31817728/170498071 [00:06<00:30, 4576417.97it/s]
     19%|#8        | 32276480/170498071 [00:06<00:30, 4578854.02it/s]
     19%|#9        | 32735232/170498071 [00:06<00:30, 4565427.11it/s]
     19%|#9        | 33193984/170498071 [00:06<00:30, 4547616.56it/s]
     20%|#9        | 33685504/170498071 [00:07<00:30, 4551320.07it/s]
     20%|##        | 34144256/170498071 [00:07<00:30, 4536192.19it/s]
     20%|##        | 34603008/170498071 [00:07<00:30, 4513326.61it/s]
     21%|##        | 35094528/170498071 [00:07<00:29, 4567622.48it/s]
     21%|##        | 35553280/170498071 [00:07<00:29, 4526758.52it/s]
     21%|##1       | 36012032/170498071 [00:07<00:29, 4527233.90it/s]
     21%|##1       | 36470784/170498071 [00:07<00:29, 4510726.23it/s]
     22%|##1       | 36929536/170498071 [00:07<00:29, 4528933.99it/s]
     22%|##1       | 37388288/170498071 [00:07<00:29, 4520375.60it/s]
     22%|##2       | 37847040/170498071 [00:07<00:29, 4530674.66it/s]
     22%|##2       | 38305792/170498071 [00:08<00:29, 4492209.93it/s]
     23%|##2       | 38764544/170498071 [00:08<00:29, 4443216.95it/s]
     23%|##3       | 39223296/170498071 [00:08<00:29, 4450311.17it/s]
     23%|##3       | 39682048/170498071 [00:08<00:29, 4416722.26it/s]
     24%|##3       | 40140800/170498071 [00:08<00:29, 4422315.93it/s]
     24%|##3       | 40599552/170498071 [00:08<00:29, 4406252.15it/s]
     24%|##4       | 41058304/170498071 [00:08<00:29, 4415717.52it/s]
     24%|##4       | 41517056/170498071 [00:08<00:29, 4425746.45it/s]
     25%|##4       | 41975808/170498071 [00:08<00:29, 4419863.54it/s]
     25%|##4       | 42434560/170498071 [00:08<00:29, 4400936.61it/s]
     25%|##5       | 42893312/170498071 [00:09<00:29, 4384945.23it/s]
     25%|##5       | 43352064/170498071 [00:09<00:29, 4378408.49it/s]
     26%|##5       | 43810816/170498071 [00:09<00:28, 4394762.19it/s]
     26%|##5       | 44269568/170498071 [00:09<00:28, 4399470.61it/s]
     26%|##6       | 44728320/170498071 [00:09<00:28, 4374459.24it/s]
     27%|##6       | 45187072/170498071 [00:09<00:28, 4391934.57it/s]
     27%|##6       | 45645824/170498071 [00:09<00:28, 4423359.09it/s]
     27%|##7       | 46104576/170498071 [00:09<00:28, 4371526.02it/s]
     27%|##7       | 46563328/170498071 [00:09<00:28, 4399581.86it/s]
     28%|##7       | 47022080/170498071 [00:10<00:27, 4412576.95it/s]
     28%|##7       | 47480832/170498071 [00:10<00:28, 4358211.77it/s]
     28%|##8       | 47939584/170498071 [00:10<00:27, 4383397.68it/s]
     28%|##8       | 48398336/170498071 [00:10<00:27, 4377232.36it/s]
     29%|##8       | 48857088/170498071 [00:10<00:28, 4300000.21it/s]
     29%|##8       | 49315840/170498071 [00:10<00:28, 4287535.43it/s]
     29%|##9       | 49774592/170498071 [00:10<00:28, 4282764.44it/s]
     29%|##9       | 50233344/170498071 [00:10<00:28, 4280662.07it/s]
     30%|##9       | 50692096/170498071 [00:10<00:27, 4308205.44it/s]
     30%|###       | 51150848/170498071 [00:10<00:27, 4360470.92it/s]
     30%|###       | 51609600/170498071 [00:11<00:27, 4370558.89it/s]
     31%|###       | 52068352/170498071 [00:11<00:26, 4432736.46it/s]
     31%|###       | 52559872/170498071 [00:11<00:26, 4497977.53it/s]
     31%|###1      | 53018624/170498071 [00:11<00:26, 4452694.82it/s]
     31%|###1      | 53510144/170498071 [00:11<00:25, 4508061.94it/s]
     32%|###1      | 53968896/170498071 [00:11<00:26, 4476428.36it/s]
     32%|###1      | 54427648/170498071 [00:11<00:25, 4498197.09it/s]
     32%|###2      | 54886400/170498071 [00:11<00:25, 4510090.37it/s]
     32%|###2      | 55345152/170498071 [00:11<00:25, 4506430.58it/s]
     33%|###2      | 55803904/170498071 [00:11<00:25, 4516365.71it/s]
     33%|###2      | 56262656/170498071 [00:12<00:25, 4525993.99it/s]
     33%|###3      | 56721408/170498071 [00:12<00:25, 4512867.04it/s]
     34%|###3      | 57180160/170498071 [00:12<00:25, 4518807.26it/s]
     34%|###3      | 57671680/170498071 [00:12<00:24, 4547777.82it/s]
     34%|###4      | 58130432/170498071 [00:12<00:24, 4512155.15it/s]
     34%|###4      | 58621952/170498071 [00:12<00:24, 4530170.15it/s]
     35%|###4      | 59080704/170498071 [00:12<00:24, 4512768.76it/s]
     35%|###4      | 59539456/170498071 [00:12<00:24, 4523734.00it/s]
     35%|###5      | 59998208/170498071 [00:12<00:24, 4533110.71it/s]
     35%|###5      | 60456960/170498071 [00:13<00:24, 4494688.96it/s]
     36%|###5      | 60948480/170498071 [00:13<00:24, 4527511.23it/s]
     36%|###6      | 61407232/170498071 [00:13<00:24, 4527801.84it/s]
     36%|###6      | 61865984/170498071 [00:13<00:24, 4526308.09it/s]
     37%|###6      | 62324736/170498071 [00:13<00:23, 4534827.37it/s]
     37%|###6      | 62783488/170498071 [00:13<00:23, 4546328.24it/s]
     37%|###7      | 63242240/170498071 [00:13<00:23, 4516151.33it/s]
     37%|###7      | 63700992/170498071 [00:13<00:23, 4512302.06it/s]
     38%|###7      | 64159744/170498071 [00:13<00:23, 4524976.28it/s]
     38%|###7      | 64618496/170498071 [00:13<00:23, 4473497.64it/s]
     38%|###8      | 65077248/170498071 [00:14<00:23, 4466194.54it/s]
     38%|###8      | 65536000/170498071 [00:14<00:23, 4453082.75it/s]
     39%|###8      | 65994752/170498071 [00:14<00:23, 4465098.16it/s]
     39%|###8      | 66453504/170498071 [00:14<00:23, 4475238.94it/s]
     39%|###9      | 66912256/170498071 [00:14<00:23, 4442325.28it/s]
     40%|###9      | 67371008/170498071 [00:14<00:23, 4466084.31it/s]
     40%|###9      | 67829760/170498071 [00:14<00:22, 4476378.39it/s]
     40%|####      | 68288512/170498071 [00:14<00:22, 4453037.73it/s]
     40%|####      | 68747264/170498071 [00:14<00:22, 4456996.04it/s]
     41%|####      | 69206016/170498071 [00:14<00:22, 4452971.95it/s]
     41%|####      | 69697536/170498071 [00:15<00:22, 4507447.73it/s]
     41%|####1     | 70156288/170498071 [00:15<00:22, 4500272.98it/s]
     41%|####1     | 70647808/170498071 [00:15<00:21, 4564164.45it/s]
     42%|####1     | 71106560/170498071 [00:15<00:21, 4570608.93it/s]
     42%|####1     | 71598080/170498071 [00:15<00:21, 4612516.68it/s]
     42%|####2     | 72089600/170498071 [00:15<00:21, 4601872.60it/s]
     43%|####2     | 72581120/170498071 [00:15<00:21, 4611633.82it/s]
     43%|####2     | 73072640/170498071 [00:15<00:21, 4622391.64it/s]
     43%|####3     | 73564160/170498071 [00:15<00:20, 4616319.32it/s]
     43%|####3     | 74055680/170498071 [00:16<00:20, 4647334.48it/s]
     44%|####3     | 74547200/170498071 [00:16<00:20, 4629242.33it/s]
     44%|####4     | 75038720/170498071 [00:16<00:20, 4690116.82it/s]
     44%|####4     | 75530240/170498071 [00:16<00:20, 4745874.39it/s]
     45%|####4     | 76054528/170498071 [00:16<00:19, 4834362.66it/s]
     45%|####4     | 76546048/170498071 [00:16<00:19, 4852196.25it/s]
     45%|####5     | 77037568/170498071 [00:16<00:19, 4798005.32it/s]
     45%|####5     | 77529088/170498071 [00:16<00:19, 4825106.98it/s]
     46%|####5     | 78053376/170498071 [00:16<00:18, 4890455.56it/s]
     46%|####6     | 78544896/170498071 [00:16<00:18, 4867524.24it/s]
     46%|####6     | 79069184/170498071 [00:17<00:18, 4908400.75it/s]
     47%|####6     | 79560704/170498071 [00:17<00:18, 4895876.85it/s]
     47%|####6     | 80052224/170498071 [00:17<00:18, 4884299.54it/s]
     47%|####7     | 80576512/170498071 [00:17<00:18, 4922948.77it/s]
     48%|####7     | 81100800/170498071 [00:17<00:17, 5012791.77it/s]
     48%|####7     | 81625088/170498071 [00:17<00:17, 5028609.87it/s]
     48%|####8     | 82149376/170498071 [00:17<00:17, 5010082.70it/s]
     48%|####8     | 82673664/170498071 [00:17<00:17, 4993809.17it/s]
     49%|####8     | 83197952/170498071 [00:17<00:17, 4991555.06it/s]
     49%|####9     | 83722240/170498071 [00:17<00:17, 4955947.73it/s]
     49%|####9     | 84246528/170498071 [00:18<00:17, 4981297.03it/s]
     50%|####9     | 84770816/170498071 [00:18<00:17, 4936166.04it/s]
     50%|#####     | 85295104/170498071 [00:18<00:17, 4986109.41it/s]
     50%|#####     | 85819392/170498071 [00:18<00:17, 4921831.09it/s]
     51%|#####     | 86343680/170498071 [00:18<00:16, 4969919.17it/s]
     51%|#####     | 86867968/170498071 [00:18<00:16, 4954930.76it/s]
     51%|#####1    | 87392256/170498071 [00:18<00:16, 4923185.39it/s]
     52%|#####1    | 87916544/170498071 [00:18<00:16, 4967454.64it/s]
     52%|#####1    | 88440832/170498071 [00:18<00:16, 4955390.59it/s]
     52%|#####2    | 88965120/170498071 [00:19<00:16, 4993694.11it/s]
     52%|#####2    | 89489408/170498071 [00:19<00:16, 5001692.11it/s]
     53%|#####2    | 90013696/170498071 [00:19<00:15, 5031915.28it/s]
     53%|#####3    | 90537984/170498071 [00:19<00:16, 4968019.14it/s]
     53%|#####3    | 91062272/170498071 [00:19<00:15, 5046252.88it/s]
     54%|#####3    | 91586560/170498071 [00:19<00:15, 5070652.23it/s]
     54%|#####4    | 92110848/170498071 [00:19<00:15, 5096227.70it/s]
     54%|#####4    | 92635136/170498071 [00:19<00:15, 5119642.52it/s]
     55%|#####4    | 93159424/170498071 [00:19<00:15, 5067397.74it/s]
     55%|#####4    | 93683712/170498071 [00:19<00:15, 5006401.19it/s]
     55%|#####5    | 94208000/170498071 [00:20<00:15, 4998338.76it/s]
     56%|#####5    | 94732288/170498071 [00:20<00:15, 4959282.75it/s]
     56%|#####5    | 95256576/170498071 [00:20<00:15, 4949372.89it/s]
     56%|#####6    | 95780864/170498071 [00:20<00:15, 4946037.97it/s]
     56%|#####6    | 96305152/170498071 [00:20<00:15, 4848531.33it/s]
     57%|#####6    | 96796672/170498071 [00:20<00:15, 4828243.75it/s]
     57%|#####7    | 97288192/170498071 [00:20<00:15, 4770586.43it/s]
     57%|#####7    | 97779712/170498071 [00:20<00:15, 4754149.58it/s]
     58%|#####7    | 98304000/170498071 [00:20<00:15, 4811729.92it/s]
     58%|#####7    | 98828288/170498071 [00:21<00:14, 4933659.78it/s]
     58%|#####8    | 99352576/170498071 [00:21<00:14, 4954703.92it/s]
     59%|#####8    | 99909632/170498071 [00:21<00:13, 5065596.84it/s]
     59%|#####8    | 100433920/170498071 [00:21<00:13, 5091776.89it/s]
     59%|#####9    | 100990976/170498071 [00:21<00:13, 5177474.51it/s]
     60%|#####9    | 101548032/170498071 [00:21<00:13, 5220017.84it/s]
     60%|#####9    | 102105088/170498071 [00:21<00:13, 5257290.50it/s]
     60%|######    | 102662144/170498071 [00:21<00:12, 5296580.20it/s]
     61%|######    | 103219200/170498071 [00:21<00:12, 5304692.17it/s]
     61%|######    | 103809024/170498071 [00:21<00:12, 5414771.67it/s]
     61%|######1   | 104431616/170498071 [00:22<00:11, 5600745.86it/s]
     62%|######1   | 105021440/170498071 [00:22<00:11, 5667993.07it/s]
     62%|######1   | 105611264/170498071 [00:22<00:11, 5712882.01it/s]
     62%|######2   | 106201088/170498071 [00:22<00:11, 5755699.47it/s]
     63%|######2   | 106823680/170498071 [00:22<00:10, 5858876.29it/s]
     63%|######2   | 107413504/170498071 [00:22<00:10, 5830265.22it/s]
     63%|######3   | 108036096/170498071 [00:22<00:10, 5862765.34it/s]
     64%|######3   | 108658688/170498071 [00:22<00:10, 5913044.85it/s]
     64%|######4   | 109281280/170498071 [00:22<00:10, 5973166.01it/s]
     64%|######4   | 109936640/170498071 [00:22<00:09, 6090268.67it/s]
     65%|######4   | 110592000/170498071 [00:23<00:09, 6159446.84it/s]
     65%|######5   | 111247360/170498071 [00:23<00:09, 6223694.07it/s]
     66%|######5   | 111902720/170498071 [00:23<00:09, 6283856.89it/s]
     66%|######6   | 112558080/170498071 [00:23<00:09, 6295475.85it/s]
     66%|######6   | 113213440/170498071 [00:23<00:09, 6293200.18it/s]
     67%|######6   | 113868800/170498071 [00:23<00:08, 6318963.02it/s]
     67%|######7   | 114524160/170498071 [00:23<00:08, 6343860.74it/s]
     68%|######7   | 115179520/170498071 [00:23<00:08, 6380549.76it/s]
     68%|######7   | 115867648/170498071 [00:23<00:08, 6443052.96it/s]
     68%|######8   | 116555776/170498071 [00:24<00:08, 6502631.33it/s]
     69%|######8   | 117243904/170498071 [00:24<00:08, 6530863.42it/s]
     69%|######9   | 117932032/170498071 [00:24<00:07, 6595070.13it/s]
     70%|######9   | 118620160/170498071 [00:24<00:07, 6604094.70it/s]
     70%|######9   | 119308288/170498071 [00:24<00:07, 6656589.20it/s]
     70%|#######   | 119996416/170498071 [00:24<00:07, 6664428.74it/s]
     71%|#######   | 120717312/170498071 [00:24<00:07, 6725517.70it/s]
     71%|#######1  | 121405440/170498071 [00:24<00:07, 6653766.96it/s]
     72%|#######1  | 122093568/170498071 [00:24<00:07, 6683950.99it/s]
     72%|#######2  | 122781696/170498071 [00:24<00:07, 6668901.77it/s]
     72%|#######2  | 123469824/170498071 [00:25<00:07, 6619147.91it/s]
     73%|#######2  | 124157952/170498071 [00:25<00:07, 6479000.38it/s]
     73%|#######3  | 124813312/170498071 [00:25<00:07, 6410636.64it/s]
     74%|#######3  | 125468672/170498071 [00:25<00:07, 6355546.09it/s]
     74%|#######3  | 126124032/170498071 [00:25<00:07, 6311808.92it/s]
     74%|#######4  | 126779392/170498071 [00:25<00:06, 6306123.27it/s]
     75%|#######4  | 127434752/170498071 [00:25<00:06, 6175306.49it/s]
     75%|#######5  | 128090112/170498071 [00:25<00:06, 6196593.67it/s]
     75%|#######5  | 128712704/170498071 [00:25<00:06, 6104157.03it/s]
     76%|#######5  | 129335296/170498071 [00:26<00:06, 5976486.31it/s]
     76%|#######6  | 129957888/170498071 [00:26<00:07, 5680430.18it/s]
     77%|#######6  | 130547712/170498071 [00:26<00:07, 5479488.06it/s]
     77%|#######6  | 131104768/170498071 [00:26<00:07, 5361127.51it/s]
     77%|#######7  | 131661824/170498071 [00:26<00:07, 5270916.29it/s]
     78%|#######7  | 132218880/170498071 [00:26<00:07, 5178074.04it/s]
     78%|#######7  | 132743168/170498071 [00:26<00:07, 5180401.97it/s]
     78%|#######8  | 133267456/170498071 [00:26<00:07, 5117324.80it/s]
     78%|#######8  | 133791744/170498071 [00:26<00:07, 5088274.43it/s]
     79%|#######8  | 134316032/170498071 [00:27<00:07, 5013679.34it/s]
     79%|#######9  | 134840320/170498071 [00:27<00:07, 4894059.45it/s]
     79%|#######9  | 135331840/170498071 [00:27<00:07, 4843690.39it/s]
     80%|#######9  | 135823360/170498071 [00:27<00:07, 4824565.76it/s]
     80%|#######9  | 136314880/170498071 [00:27<00:07, 4801215.81it/s]
     80%|########  | 136806400/170498071 [00:27<00:07, 4811813.42it/s]
     81%|########  | 137297920/170498071 [00:27<00:06, 4758528.24it/s]
     81%|########  | 137789440/170498071 [00:27<00:06, 4711431.35it/s]
     81%|########1 | 138280960/170498071 [00:27<00:06, 4605369.38it/s]
     81%|########1 | 138772480/170498071 [00:27<00:06, 4552971.76it/s]
     82%|########1 | 139231232/170498071 [00:28<00:06, 4466993.56it/s]
     82%|########1 | 139689984/170498071 [00:28<00:07, 4366750.23it/s]
     82%|########2 | 140148736/170498071 [00:28<00:07, 4333756.19it/s]
     82%|########2 | 140607488/170498071 [00:28<00:06, 4319607.68it/s]
     83%|########2 | 141066240/170498071 [00:28<00:06, 4262818.93it/s]
     83%|########3 | 141524992/170498071 [00:28<00:06, 4275403.25it/s]
     83%|########3 | 141983744/170498071 [00:28<00:06, 4260637.15it/s]
     84%|########3 | 142442496/170498071 [00:28<00:06, 4208545.40it/s]
     84%|########3 | 142868480/170498071 [00:28<00:06, 4155011.47it/s]
     84%|########4 | 143294464/170498071 [00:29<00:06, 4085700.03it/s]
     84%|########4 | 143720448/170498071 [00:29<00:06, 3959278.76it/s]
     85%|########4 | 144146432/170498071 [00:29<00:06, 3882984.74it/s]
     85%|########4 | 144539648/170498071 [00:29<00:06, 3808113.81it/s]
     85%|########5 | 144932864/170498071 [00:29<00:06, 3720668.58it/s]
     85%|########5 | 145326080/170498071 [00:29<00:06, 3666057.16it/s]
     85%|########5 | 145719296/170498071 [00:29<00:06, 3650661.19it/s]
     86%|########5 | 146112512/170498071 [00:29<00:06, 3592868.80it/s]
     86%|########5 | 146472960/170498071 [00:29<00:06, 3582013.69it/s]
     86%|########6 | 146833408/170498071 [00:30<00:06, 3553414.13it/s]
     86%|########6 | 147193856/170498071 [00:30<00:06, 3547032.22it/s]
     87%|########6 | 147554304/170498071 [00:30<00:06, 3560229.00it/s]
     87%|########6 | 147914752/170498071 [00:30<00:06, 3533507.39it/s]
     87%|########6 | 148275200/170498071 [00:30<00:06, 3517061.88it/s]
     87%|########7 | 148635648/170498071 [00:30<00:06, 3418504.38it/s]
     87%|########7 | 148996096/170498071 [00:30<00:06, 3420486.13it/s]
     88%|########7 | 149356544/170498071 [00:30<00:06, 3437263.71it/s]
     88%|########7 | 149716992/170498071 [00:30<00:06, 3434962.84it/s]
     88%|########8 | 150077440/170498071 [00:30<00:05, 3427818.31it/s]
     88%|########8 | 150437888/170498071 [00:31<00:05, 3441749.57it/s]
     88%|########8 | 150798336/170498071 [00:31<00:05, 3442798.59it/s]
     89%|########8 | 151158784/170498071 [00:31<00:05, 3454646.46it/s]
     89%|########8 | 151519232/170498071 [00:31<00:05, 3417080.31it/s]
     89%|########9 | 151879680/170498071 [00:31<00:05, 3434797.33it/s]
     89%|########9 | 152240128/170498071 [00:31<00:05, 3390359.48it/s]
     90%|########9 | 152600576/170498071 [00:31<00:05, 3417326.84it/s]
     90%|########9 | 152961024/170498071 [00:31<00:05, 3423101.21it/s]
     90%|########9 | 153321472/170498071 [00:31<00:05, 3401464.49it/s]
     90%|######### | 153681920/170498071 [00:32<00:04, 3402105.11it/s]
     90%|######### | 154042368/170498071 [00:32<00:04, 3398652.84it/s]
     91%|######### | 154402816/170498071 [00:32<00:04, 3373713.61it/s]
     91%|######### | 154763264/170498071 [00:32<00:04, 3333206.72it/s]
     91%|######### | 155123712/170498071 [00:32<00:04, 3325110.68it/s]
     91%|#########1| 155484160/170498071 [00:32<00:04, 3301510.41it/s]
     91%|#########1| 155844608/170498071 [00:32<00:04, 3275077.10it/s]
     92%|#########1| 156172288/170498071 [00:32<00:04, 3271767.76it/s]
     92%|#########1| 156499968/170498071 [00:32<00:04, 3228135.75it/s]
     92%|#########1| 156827648/170498071 [00:33<00:04, 3229824.14it/s]
     92%|#########2| 157155328/170498071 [00:33<00:04, 3221130.42it/s]
     92%|#########2| 157483008/170498071 [00:33<00:04, 3165608.02it/s]
     93%|#########2| 157810688/170498071 [00:33<00:04, 3162476.06it/s]
     93%|#########2| 158138368/170498071 [00:33<00:03, 3159205.33it/s]
     93%|#########2| 158466048/170498071 [00:33<00:03, 3131431.09it/s]
     93%|#########3| 158793728/170498071 [00:33<00:03, 3162533.96it/s]
     93%|#########3| 159121408/170498071 [00:33<00:03, 3166031.70it/s]
     94%|#########3| 159449088/170498071 [00:33<00:03, 3163683.37it/s]
     94%|#########3| 159776768/170498071 [00:33<00:03, 3121154.54it/s]
     94%|#########3| 160104448/170498071 [00:34<00:03, 3120966.59it/s]
     94%|#########4| 160432128/170498071 [00:34<00:03, 3137711.46it/s]
     94%|#########4| 160759808/170498071 [00:34<00:03, 3149818.76it/s]
     94%|#########4| 161087488/170498071 [00:34<00:02, 3139343.64it/s]
     95%|#########4| 161415168/170498071 [00:34<00:02, 3143047.68it/s]
     95%|#########4| 161742848/170498071 [00:34<00:02, 3139873.53it/s]
     95%|#########5| 162070528/170498071 [00:34<00:02, 3113368.97it/s]
     95%|#########5| 162398208/170498071 [00:34<00:02, 3129269.73it/s]
     95%|#########5| 162725888/170498071 [00:34<00:02, 3118650.32it/s]
     96%|#########5| 163053568/170498071 [00:35<00:02, 3107964.56it/s]
     96%|#########5| 163381248/170498071 [00:35<00:02, 3103806.73it/s]
     96%|#########6| 163708928/170498071 [00:35<00:02, 3111086.12it/s]
     96%|#########6| 164036608/170498071 [00:35<00:02, 3090436.81it/s]
     96%|#########6| 164364288/170498071 [00:35<00:01, 3070567.51it/s]
     97%|#########6| 164691968/170498071 [00:35<00:01, 3054582.58it/s]
     97%|#########6| 165019648/170498071 [00:35<00:01, 3041741.80it/s]
     97%|#########6| 165347328/170498071 [00:35<00:01, 3000550.90it/s]
     97%|#########7| 165675008/170498071 [00:35<00:01, 3009064.24it/s]
     97%|#########7| 166002688/170498071 [00:35<00:01, 3013071.40it/s]
     98%|#########7| 166330368/170498071 [00:36<00:01, 2978044.06it/s]
     98%|#########7| 166658048/170498071 [00:36<00:01, 2971223.15it/s]
     98%|#########7| 166985728/170498071 [00:36<00:01, 2974931.31it/s]
     98%|#########8| 167313408/170498071 [00:36<00:01, 2947737.57it/s]
     98%|#########8| 167641088/170498071 [00:36<00:00, 2987447.19it/s]
     99%|#########8| 167968768/170498071 [00:36<00:00, 2999666.45it/s]
     99%|#########8| 168296448/170498071 [00:36<00:00, 3005029.66it/s]
     99%|#########8| 168624128/170498071 [00:36<00:00, 3016454.17it/s]
     99%|#########9| 168951808/170498071 [00:36<00:00, 3031953.29it/s]
     99%|#########9| 169279488/170498071 [00:37<00:00, 2865037.84it/s]
    100%|#########9| 169672704/170498071 [00:37<00:00, 3150331.32it/s]
    100%|#########9| 170000384/170498071 [00:37<00:00, 3168523.66it/s]
    100%|#########9| 170328064/170498071 [00:37<00:00, 3172555.21it/s]
    100%|##########| 170498071/170498071 [00:37<00:00, 4552680.74it/s]
    Extracting ./data/cifar-10-python.tar.gz to ./data
    Files already downloaded and verified




.. GENERATED FROM PYTHON SOURCE LINES 80-88

.. note:: This section is for CPU users only who are interested in quick results. Use this option only if you're interested in a small scale experiment. Keep in mind the code should run fairly quickly using any GPU. Select only the first ``num_images_to_keep`` images from the train/test dataset

   .. code-block:: python

      #from torch.utils.data import Subset
      #num_images_to_keep = 2000
      #train_dataset = Subset(train_dataset, range(min(num_images_to_keep, 50_000)))
      #test_dataset = Subset(test_dataset, range(min(num_images_to_keep, 10_000)))

.. GENERATED FROM PYTHON SOURCE LINES 88-93

.. code-block:: default


    #Dataloaders
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)








.. GENERATED FROM PYTHON SOURCE LINES 94-99

Defining model classes and utility functions
--------------------------------------------
Next, we need to define our model classes. Several user-defined parameters need to be set here. We use two different architectures, keeping the number of filters fixed across our experiments to ensure fair comparisons.
Both architectures are Convolutional Neural Networks (CNNs) with a different number of convolutional layers that serve as feature extractors, followed by a classifier with 10 classes. 
The number of filters and neurons is smaller for the students.

.. GENERATED FROM PYTHON SOURCE LINES 99-154

.. code-block:: default


    # Deeper neural network class to be used as teacher:
    class DeepNN(nn.Module):
        def __init__(self, num_classes=10):
            super(DeepNN, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 128, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Conv2d(128, 64, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Conv2d(64, 64, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 32, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )
            self.classifier = nn.Sequential(
                nn.Linear(2048, 512),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(512, num_classes)
            )

        def forward(self, x):
            x = self.features(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x

    # Lightweight neural network class to be used as student:
    class LightNN(nn.Module):
        def __init__(self, num_classes=10):
            super(LightNN, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 16, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Conv2d(16, 16, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )
            self.classifier = nn.Sequential(
                nn.Linear(1024, 256),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(256, num_classes)
            )

        def forward(self, x):
            x = self.features(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x








.. GENERATED FROM PYTHON SOURCE LINES 155-171

We employ 2 functions to help us produce and evaluate the results on our original classification task.
One function is called ``train`` and takes the following arguments:

- ``model``: A model instance to train (update its weights) via this function.
- ``train_loader``: We defined our ``train_loader`` above, and its job is to feed the data into the model.
- ``epochs``: How many times we loop over the dataset.
- ``learning_rate``: The learning rate determines how large our steps towards convergence should be. Too large or too small steps can be detrimental.
- ``device``: Determines the device to run the workload on. Can be either CPU or GPU depending on availability.

Our test function is similar, but it will be invoked with ``test_loader`` to load images from the test set.

.. figure:: /../_static/img/knowledge_distillation/ce_only.png 
   :align: center

   Train both networks with Cross-Entropy. The student will be used as a baseline:


.. GENERATED FROM PYTHON SOURCE LINES 171-219

.. code-block:: default


    def train(model, train_loader, epochs, learning_rate, device):
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)

        model.train()

        for epoch in range(epochs):
            running_loss = 0.0
            for inputs, labels in train_loader:
                # inputs: A collection of batch_size images
                # labels: A vector of dimensionality batch_size with integers denoting class of each image
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()
                outputs = model(inputs)

                # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes
                # labels: The actual labels of the images. Vector of dimensionality batch_size
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                running_loss += loss.item()

            print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}")

    def test(model, test_loader, device):
        model.to(device)
        model.eval()

        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)

                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        accuracy = 100 * correct / total
        print(f"Test Accuracy: {accuracy:.2f}%")
        return accuracy








.. GENERATED FROM PYTHON SOURCE LINES 220-225

Cross-entropy runs
------------------
For reproducibility, we need to set the torch manual seed. We train networks using different methods, so to compare them fairly,
it makes sense to initialize the networks with the same weights.
Start by training the teacher network using cross-entropy:

.. GENERATED FROM PYTHON SOURCE LINES 225-235

.. code-block:: default


    torch.manual_seed(42)
    nn_deep = DeepNN(num_classes=10).to(device)
    train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)
    test_accuracy_deep = test(nn_deep, test_loader, device)

    # Instantiate the lightweight network:
    torch.manual_seed(42)
    nn_light = LightNN(num_classes=10).to(device)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Epoch 1/10, Loss: 1.3275167399355212
    Epoch 2/10, Loss: 0.865842379724888
    Epoch 3/10, Loss: 0.6771407182259328
    Epoch 4/10, Loss: 0.5306401341162679
    Epoch 5/10, Loss: 0.40612332488569763
    Epoch 6/10, Loss: 0.3020361427532133
    Epoch 7/10, Loss: 0.2132949793849455
    Epoch 8/10, Loss: 0.15886043309403197
    Epoch 9/10, Loss: 0.13886675183349254
    Epoch 10/10, Loss: 0.12016757338038643
    Test Accuracy: 75.06%




.. GENERATED FROM PYTHON SOURCE LINES 236-239

We instantiate one more lightweight network model to compare their performances.
Back propagation is sensitive to weight initialization,
so we need to make sure these two networks have the exact same initialization.

.. GENERATED FROM PYTHON SOURCE LINES 239-243

.. code-block:: default


    torch.manual_seed(42)
    new_nn_light = LightNN(num_classes=10).to(device)








.. GENERATED FROM PYTHON SOURCE LINES 244-246

To ensure we have created a copy of the first network, we inspect the norm of its first layer.
If it matches, then we are safe to conclude that the networks are indeed the same.

.. GENERATED FROM PYTHON SOURCE LINES 246-252

.. code-block:: default


    # Print the norm of the first layer of the initial lightweight model
    print("Norm of 1st layer of nn_light:", torch.norm(nn_light.features[0].weight).item())
    # Print the norm of the first layer of the new lightweight model
    print("Norm of 1st layer of new_nn_light:", torch.norm(new_nn_light.features[0].weight).item())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Norm of 1st layer of nn_light: 2.327361822128296
    Norm of 1st layer of new_nn_light: 2.327361822128296




.. GENERATED FROM PYTHON SOURCE LINES 253-254

Print the total number of parameters in each model:

.. GENERATED FROM PYTHON SOURCE LINES 254-259

.. code-block:: default

    total_params_deep = "{:,}".format(sum(p.numel() for p in nn_deep.parameters()))
    print(f"DeepNN parameters: {total_params_deep}")
    total_params_light = "{:,}".format(sum(p.numel() for p in nn_light.parameters()))
    print(f"LightNN parameters: {total_params_light}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    DeepNN parameters: 1,186,986
    LightNN parameters: 267,738




.. GENERATED FROM PYTHON SOURCE LINES 260-261

Train and test the lightweight network with cross entropy loss:

.. GENERATED FROM PYTHON SOURCE LINES 261-264

.. code-block:: default

    train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)
    test_accuracy_light_ce = test(nn_light, test_loader, device)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Epoch 1/10, Loss: 1.4696215683846827
    Epoch 2/10, Loss: 1.157501962636133
    Epoch 3/10, Loss: 1.0249026960424146
    Epoch 4/10, Loss: 0.9239688681824433
    Epoch 5/10, Loss: 0.8499201234344327
    Epoch 6/10, Loss: 0.7840954623258937
    Epoch 7/10, Loss: 0.7154864179508765
    Epoch 8/10, Loss: 0.6608714752489954
    Epoch 9/10, Loss: 0.6077478013532546
    Epoch 10/10, Loss: 0.5560628478331944
    Test Accuracy: 70.25%




.. GENERATED FROM PYTHON SOURCE LINES 265-267

As we can see, based on test accuracy, we can now compare the deeper network that is to be used as a teacher with the lightweight network that is our supposed student. So far, our student has not intervened with the teacher, therefore this performance is achieved by the student itself.
The metrics so far can be seen with the following lines:

.. GENERATED FROM PYTHON SOURCE LINES 267-271

.. code-block:: default


    print(f"Teacher accuracy: {test_accuracy_deep:.2f}%")
    print(f"Student accuracy: {test_accuracy_light_ce:.2f}%")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Teacher accuracy: 75.06%
    Student accuracy: 70.25%




.. GENERATED FROM PYTHON SOURCE LINES 272-300

Knowledge distillation run
--------------------------
Now let's try to improve the test accuracy of the student network by incorporating the teacher.
Knowledge distillation is a straightforward technique to achieve this,
based on the fact that both networks output a probability distribution over our classes.
Therefore, the two networks share the same number of output neurons.
The method works by incorporating an additional loss into the traditional cross entropy loss,
which is based on the softmax output of the teacher network.
The assumption is that the output activations of a properly trained teacher network carry additional information that can be leveraged by a student network during training.
The original work suggests that utilizing ratios of smaller probabilities in the soft targets can help achieve the underlying objective of deep neural networks,
which is to create a similarity structure over the data where similar objects are mapped closer together.
For example, in CIFAR-10, a truck could be mistaken for an automobile or airplane,
if its wheels are present, but it is less likely to be mistaken for a dog. 
Therefore, it makes sense to assume that valuable information resides not only in the top prediction of a properly trained model but in the entire output distribution.
However, cross entropy alone does not sufficiently exploit this information as the activations for non-predicted classes
tend to be so small that propagated gradients do not meaningfully change the weights to construct this desirable vector space.

As we continue defining our first helper function that introduces a teacher-student dynamic, we need to include a few extra parameters:

- ``T``: Temperature controls the smoothness of the output distributions. Larger ``T`` leads to smoother distributions, thus smaller probabilities get a larger boost.
- ``soft_target_loss_weight``: A weight assigned to the extra objective we're about to include.
- ``ce_loss_weight``: A weight assigned to cross-entropy. Tuning these weights pushes the network towards optimizing for either objective.

.. figure:: /../_static/img/knowledge_distillation/distillation_output_loss.png 
   :align: center

   Distillation loss is calculated from the logits of the networks. It only returns gradients to the student:


.. GENERATED FROM PYTHON SOURCE LINES 300-351

.. code-block:: default


    def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):
        ce_loss = nn.CrossEntropyLoss()
        optimizer = optim.Adam(student.parameters(), lr=learning_rate)

        teacher.eval()  # Teacher set to evaluation mode
        student.train() # Student to train mode

        for epoch in range(epochs):
            running_loss = 0.0
            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()

                # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights
                with torch.no_grad():
                    teacher_logits = teacher(inputs)

                # Forward pass with the student model
                student_logits = student(inputs)

                #Soften the student logits by applying softmax first and log() second
                soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)
                soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)

                # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper "Distilling the knowledge in a neural network"
                soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)

                # Calculate the true label loss
                label_loss = ce_loss(student_logits, labels)

                # Weighted sum of the two losses
                loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss

                loss.backward()
                optimizer.step()

                running_loss += loss.item()

            print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}")

    # Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.
    train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)
    test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)

    # Compare the student test accuracy with and without the teacher, after distillation
    print(f"Teacher accuracy: {test_accuracy_deep:.2f}%")
    print(f"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%")
    print(f"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Epoch 1/10, Loss: 2.3994315944974076
    Epoch 2/10, Loss: 1.8833903659640066
    Epoch 3/10, Loss: 1.656663294033626
    Epoch 4/10, Loss: 1.497435332564137
    Epoch 5/10, Loss: 1.372104554834878
    Epoch 6/10, Loss: 1.259037159288021
    Epoch 7/10, Loss: 1.1612001611753497
    Epoch 8/10, Loss: 1.0752563805836242
    Epoch 9/10, Loss: 1.0013417093955037
    Epoch 10/10, Loss: 0.9332666240079933
    Test Accuracy: 70.68%
    Teacher accuracy: 75.06%
    Student accuracy without teacher: 70.25%
    Student accuracy with CE + KD: 70.68%




.. GENERATED FROM PYTHON SOURCE LINES 352-384

Cosine loss minimization run
----------------------------
Feel free to play around with the temperature parameter that controls the softness of the softmax function and the loss coefficients.
In neural networks, it is easy to include additional loss functions to the main objectives to achieve goals like better generalization.
Let's try including an objective for the student, but now let's focus on their hidden states rather than their output layers.
Our goal is to convey information from the teacher's representation to the student by including a naive loss function,
whose minimization implies that the flattened vectors that are subsequently passed to the classifiers have become more *similar* as the loss decreases.
Of course, the teacher does not update its weights, so the minimization depends only on the student's weights.
The rationale behind this method is that we are operating under the assumption that the teacher model has a better internal representation that is
unlikely to be achieved by the student without external intervention, therefore we artificially push the student to mimic the internal representation of the teacher.
Whether or not this will end up helping the student is not straightforward, though, because pushing the lightweight network
to reach this point could be a good thing, assuming that we have found an internal representation that leads to better test accuracy,
but it could also be harmful because the networks have different architectures and the student does not have the same learning capacity as the teacher.
In other words, there is no reason for these two vectors, the student's and the teacher's to match per component.
The student could reach an internal representation that is a permutation of the teacher's and it would be just as efficient.
Nonetheless, we can still run a quick experiment to figure out the impact of this method.
We will be using the ``CosineEmbeddingLoss`` which is given by the following formula:

.. figure:: /../_static/img/knowledge_distillation/cosine_embedding_loss.png 
   :align: center
   :width: 450px

   Formula for CosineEmbeddingLoss

Obviously, there is one thing that we need to resolve first.
When we applied distillation to the output layer we mentioned that both networks have the same number of neurons, equal to the number of classes.
However, this is not the case for the layer following our convolutional layers. Here, the teacher has more neurons than the student
after the flattening of the final convolutional layer. Our loss function accepts two vectors of equal dimensionality as inputs,
therefore we need to somehow match them. We will solve this by including an average pooling layer after the teacher's convolutional layer to reduce its dimensionality to match that of the student.

To proceed, we will modify our model classes, or create new ones.
Now, the forward function returns not only the logits of the network but also the flattened hidden representation after the convolutional layer. We include the aforementioned pooling for the modified teacher.

.. GENERATED FROM PYTHON SOURCE LINES 384-452

.. code-block:: default


    class ModifiedDeepNNCosine(nn.Module):
        def __init__(self, num_classes=10):
            super(ModifiedDeepNNCosine, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 128, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Conv2d(128, 64, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Conv2d(64, 64, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 32, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )
            self.classifier = nn.Sequential(
                nn.Linear(2048, 512),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(512, num_classes)
            )

        def forward(self, x):
            x = self.features(x)
            flattened_conv_output = torch.flatten(x, 1)
            x = self.classifier(flattened_conv_output)
            flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)
            return x, flattened_conv_output_after_pooling

    # Create a similar student class where we return a tuple. We do not apply pooling after flattening.
    class ModifiedLightNNCosine(nn.Module):
        def __init__(self, num_classes=10):
            super(ModifiedLightNNCosine, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 16, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Conv2d(16, 16, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )
            self.classifier = nn.Sequential(
                nn.Linear(1024, 256),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(256, num_classes)
            )

        def forward(self, x):
            x = self.features(x)
            flattened_conv_output = torch.flatten(x, 1)
            x = self.classifier(flattened_conv_output)
            return x, flattened_conv_output

    # We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance
    modified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)
    modified_nn_deep.load_state_dict(nn_deep.state_dict())

    # Once again ensure the norm of the first layer is the same for both networks
    print("Norm of 1st layer for deep_nn:", torch.norm(nn_deep.features[0].weight).item())
    print("Norm of 1st layer for modified_deep_nn:", torch.norm(modified_nn_deep.features[0].weight).item())

    # Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.
    torch.manual_seed(42)
    modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)
    print("Norm of 1st layer:", torch.norm(modified_nn_light.features[0].weight).item())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Norm of 1st layer for deep_nn: 7.486268520355225
    Norm of 1st layer for modified_deep_nn: 7.486268520355225
    Norm of 1st layer: 2.327361822128296




.. GENERATED FROM PYTHON SOURCE LINES 453-455

Naturally, we need to change the train loop because now the model returns a tuple ``(logits, hidden_representation)``. Using a sample input tensor
we can print their shapes.

.. GENERATED FROM PYTHON SOURCE LINES 455-473

.. code-block:: default


    # Create a sample input tensor
    sample_input = torch.randn(128, 3, 32, 32).to(device) # Batch size: 128, Filters: 3, Image size: 32x32

    # Pass the input through the student
    logits, hidden_representation = modified_nn_light(sample_input)

    # Print the shapes of the tensors
    print("Student logits shape:", logits.shape) # batch_size x total_classes
    print("Student hidden representation shape:", hidden_representation.shape) # batch_size x hidden_representation_size

    # Pass the input through the teacher
    logits, hidden_representation = modified_nn_deep(sample_input)

    # Print the shapes of the tensors
    print("Teacher logits shape:", logits.shape) # batch_size x total_classes
    print("Teacher hidden representation shape:", hidden_representation.shape) # batch_size x hidden_representation_size





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Student logits shape: torch.Size([128, 10])
    Student hidden representation shape: torch.Size([128, 1024])
    Teacher logits shape: torch.Size([128, 10])
    Teacher hidden representation shape: torch.Size([128, 1024])




.. GENERATED FROM PYTHON SOURCE LINES 474-484

In our case, ``hidden_representation_size`` is ``1024``. This is the flattened feature map of the final convolutional layer of the student and as you can see,
it is the input for its classifier. It is ``1024`` for the teacher too, because we made it so with ``avg_pool1d`` from ``2048``.
The loss applied here only affects the weights of the student prior to the loss calculation. In other words, it does not affect the classifier of the student.
The modified training loop is the following:

.. figure:: /../_static/img/knowledge_distillation/cosine_loss_distillation.png 
   :align: center

   In Cosine Loss minimization, we want to maximize the cosine similarity of the two representations by returning gradients to the student:


.. GENERATED FROM PYTHON SOURCE LINES 484-525

.. code-block:: default


    def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):
        ce_loss = nn.CrossEntropyLoss()
        cosine_loss = nn.CosineEmbeddingLoss()
        optimizer = optim.Adam(student.parameters(), lr=learning_rate)

        teacher.to(device)
        student.to(device)
        teacher.eval()  # Teacher set to evaluation mode
        student.train() # Student to train mode

        for epoch in range(epochs):
            running_loss = 0.0
            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()

                # Forward pass with the teacher model and keep only the hidden representation
                with torch.no_grad():
                    _, teacher_hidden_representation = teacher(inputs)

                # Forward pass with the student model
                student_logits, student_hidden_representation = student(inputs)

                # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.
                hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))

                # Calculate the true label loss
                label_loss = ce_loss(student_logits, labels)

                # Weighted sum of the two losses
                loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss

                loss.backward()
                optimizer.step()

                running_loss += loss.item()

            print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}")








.. GENERATED FROM PYTHON SOURCE LINES 526-527

We need to modify our test function for the same reason. Here we ignore the hidden representation returned by the model.

.. GENERATED FROM PYTHON SOURCE LINES 527-549

.. code-block:: default


    def test_multiple_outputs(model, test_loader, device):
        model.to(device)
        model.eval()

        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)

                outputs, _ = model(inputs) # Disregard the second tensor of the tuple
                _, predicted = torch.max(outputs.data, 1)

                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        accuracy = 100 * correct / total
        print(f"Test Accuracy: {accuracy:.2f}%")
        return accuracy








.. GENERATED FROM PYTHON SOURCE LINES 550-552

In this case, we could easily include both knowledge distillation and cosine loss minimization in the same function. It is common to combine methods to achieve better performance in teacher-student paradigms.
For now, we can run a simple train-test session.

.. GENERATED FROM PYTHON SOURCE LINES 552-557

.. code-block:: default


    # Train and test the lightweight network with cross entropy loss
    train_cosine_loss(teacher=modified_nn_deep, student=modified_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, hidden_rep_loss_weight=0.25, ce_loss_weight=0.75, device=device)
    test_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_nn_light, test_loader, device)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Epoch 1/10, Loss: 1.3071088504303447
    Epoch 2/10, Loss: 1.0694850924069925
    Epoch 3/10, Loss: 0.966661820631198
    Epoch 4/10, Loss: 0.8891611960537903
    Epoch 5/10, Loss: 0.8349067593169639
    Epoch 6/10, Loss: 0.7887912202064339
    Epoch 7/10, Loss: 0.7475906843724458
    Epoch 8/10, Loss: 0.7130289646365758
    Epoch 9/10, Loss: 0.6736775966708922
    Epoch 10/10, Loss: 0.6488786385492291
    Test Accuracy: 71.07%




.. GENERATED FROM PYTHON SOURCE LINES 558-574

Intermediate regressor run
--------------------------
Our naive minimization does not guarantee better results for several reasons, one being the dimensionality of the vectors.
Cosine similarity generally works better than Euclidean distance for vectors of higher dimensionality,
but we were dealing with vectors with 1024 components each, so it is much harder to extract meaningful similarities.
Furthermore, as we mentioned, pushing towards a match of the hidden representation of the teacher and the student is not supported by theory.
There are no good reasons why we should be aiming for a 1:1 match of these vectors.
We will provide a final example of training intervention by including an extra network called regressor.
The objective is to first extract the feature map of the teacher after a convolutional layer,
then extract a feature map of the student after a convolutional layer, and finally try to match these maps.
However, this time, we will introduce a regressor between the networks to facilitate the matching process.
The regressor will be trainable and ideally will do a better job than our naive cosine loss minimization scheme.
Its main job is to match the dimensionality of these feature maps so that we can properly define a loss function between the teacher and the student.
Defining such a loss function provides a teaching "path," which is basically a flow to back-propagate gradients that will change the student's weights.
Focusing on the output of the convolutional layers right before each classifier for our original networks, we have the following shapes:


.. GENERATED FROM PYTHON SOURCE LINES 574-583

.. code-block:: default


    # Pass the sample input only from the convolutional feature extractor
    convolutional_fe_output_student = nn_light.features(sample_input)
    convolutional_fe_output_teacher = nn_deep.features(sample_input)

    # Print their shapes
    print("Student's feature extractor output shape: ", convolutional_fe_output_student.shape)
    print("Teacher's feature extractor output shape: ", convolutional_fe_output_teacher.shape)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Student's feature extractor output shape:  torch.Size([128, 16, 8, 8])
    Teacher's feature extractor output shape:  torch.Size([128, 32, 8, 8])




.. GENERATED FROM PYTHON SOURCE LINES 584-594

We have 32 filters for the teacher and 16 filters for the student.
We will include a trainable layer that converts the feature map of the student to the shape of the feature map of the teacher.
In practice, we modify the lightweight class to return the hidden state after an intermediate regressor that matches the sizes of the convolutional
feature maps and the teacher class to return the output of the final convolutional layer without pooling or flattening.

.. figure:: /../_static/img/knowledge_distillation/fitnets_knowledge_distill.png 
   :align: center

   The trainable layer matches the shapes of the intermediate tensors and Mean Squared Error (MSE) is properly defined:


.. GENERATED FROM PYTHON SOURCE LINES 594-653

.. code-block:: default


    class ModifiedDeepNNRegressor(nn.Module):
        def __init__(self, num_classes=10):
            super(ModifiedDeepNNRegressor, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 128, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Conv2d(128, 64, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Conv2d(64, 64, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 32, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )
            self.classifier = nn.Sequential(
                nn.Linear(2048, 512),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(512, num_classes)
            )

        def forward(self, x):
            x = self.features(x)
            conv_feature_map = x
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x, conv_feature_map

    class ModifiedLightNNRegressor(nn.Module):
        def __init__(self, num_classes=10):
            super(ModifiedLightNNRegressor, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 16, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Conv2d(16, 16, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )
            # Include an extra regressor (in our case linear)
            self.regressor = nn.Sequential(
                nn.Conv2d(16, 32, kernel_size=3, padding=1)
            )
            self.classifier = nn.Sequential(
                nn.Linear(1024, 256),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(256, num_classes)
            )

        def forward(self, x):
            x = self.features(x)
            regressor_output = self.regressor(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x, regressor_output








.. GENERATED FROM PYTHON SOURCE LINES 654-657

After that, we have to update our train loop again. This time, we extract the regressor output of the student, the feature map of the teacher,
we calculate the ``MSE`` on these tensors (they have the exact same shape so it's properly defined) and we back propagate gradients based on that loss,
in addition to the regular cross entropy loss of the classification task.

.. GENERATED FROM PYTHON SOURCE LINES 657-712

.. code-block:: default


    def train_mse_loss(teacher, student, train_loader, epochs, learning_rate, feature_map_weight, ce_loss_weight, device):
        ce_loss = nn.CrossEntropyLoss()
        mse_loss = nn.MSELoss()
        optimizer = optim.Adam(student.parameters(), lr=learning_rate)

        teacher.to(device)
        student.to(device)
        teacher.eval()  # Teacher set to evaluation mode
        student.train() # Student to train mode

        for epoch in range(epochs):
            running_loss = 0.0
            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()

                # Again ignore teacher logits
                with torch.no_grad():
                    _, teacher_feature_map = teacher(inputs)

                # Forward pass with the student model
                student_logits, regressor_feature_map = student(inputs)

                # Calculate the loss
                hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)

                # Calculate the true label loss
                label_loss = ce_loss(student_logits, labels)

                # Weighted sum of the two losses
                loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss

                loss.backward()
                optimizer.step()

                running_loss += loss.item()

            print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}")

    # Notice how our test function remains the same here with the one we used in our previous case. We only care about the actual outputs because we measure accuracy.

    # Initialize a ModifiedLightNNRegressor
    torch.manual_seed(42)
    modified_nn_light_reg = ModifiedLightNNRegressor(num_classes=10).to(device)

    # We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance
    modified_nn_deep_reg = ModifiedDeepNNRegressor(num_classes=10).to(device)
    modified_nn_deep_reg.load_state_dict(nn_deep.state_dict())

    # Train and test once again
    train_mse_loss(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, train_loader=train_loader, epochs=10, learning_rate=0.001, feature_map_weight=0.25, ce_loss_weight=0.75, device=device)
    test_accuracy_light_ce_and_mse_loss = test_multiple_outputs(modified_nn_light_reg, test_loader, device)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Epoch 1/10, Loss: 1.7069828284670934
    Epoch 2/10, Loss: 1.3354787850928733
    Epoch 3/10, Loss: 1.189067784020358
    Epoch 4/10, Loss: 1.094220961603667
    Epoch 5/10, Loss: 1.0167346273541755
    Epoch 6/10, Loss: 0.9537706027555344
    Epoch 7/10, Loss: 0.8990200219861687
    Epoch 8/10, Loss: 0.8508770424691613
    Epoch 9/10, Loss: 0.808205549979149
    Epoch 10/10, Loss: 0.7703896457581874
    Test Accuracy: 70.83%




.. GENERATED FROM PYTHON SOURCE LINES 713-716

It is expected that the final method will work better than ``CosineLoss`` because now we have allowed a trainable layer between the teacher and the student,
which gives the student some wiggle room when it comes to learning, rather than pushing the student to copy the teacher's representation.
Including the extra network is the idea behind hint-based distillation.

.. GENERATED FROM PYTHON SOURCE LINES 716-723

.. code-block:: default


    print(f"Teacher accuracy: {test_accuracy_deep:.2f}%")
    print(f"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%")
    print(f"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%")
    print(f"Student accuracy with CE + CosineLoss: {test_accuracy_light_ce_and_cosine_loss:.2f}%")
    print(f"Student accuracy with CE + RegressorMSE: {test_accuracy_light_ce_and_mse_loss:.2f}%")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Teacher accuracy: 75.06%
    Student accuracy without teacher: 70.25%
    Student accuracy with CE + KD: 70.68%
    Student accuracy with CE + CosineLoss: 71.07%
    Student accuracy with CE + RegressorMSE: 70.83%




.. GENERATED FROM PYTHON SOURCE LINES 724-739

Conclusion
--------------------------------------------
None of the methods above increases the number of parameters for the network or inference time,
so the performance increase comes at the little cost of calculating gradients during training.
In ML applications, we mostly care about inference time because training happens before the model deployment.
If our lightweight model is still too heavy for deployment, we can apply different ideas, such as post-training quantization.
Additional losses can be applied in many tasks, not just classification, and you can experiment with quantities like coefficients,
temperature, or number of neurons. Feel free to tune any numbers in the tutorial above,
but keep in mind, if you change the number of neurons / filters chances are a shape mismatch might occur.

For more information, see:

* `Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In: Neural Information Processing System Deep Learning Workshop (2015) <https://arxiv.org/abs/1503.02531>`_

* `Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets: Hints for thin deep nets. In: Proceedings of the International Conference on Learning Representations (2015) <https://arxiv.org/abs/1412.6550>`_


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 8 minutes  23.346 seconds)


.. _sphx_glr_download_beginner_knowledge_distillation_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: knowledge_distillation_tutorial.py <knowledge_distillation_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: knowledge_distillation_tutorial.ipynb <knowledge_distillation_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
